{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al modelo guardado y al archivo CSV con tus datos de ECG\n",
    "ruta_modelo = '../modelo_ecg.h5'\n",
    "ruta_csv = '../data/sensor_data.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitud fija para cada latido (asegúrate de que tus datos coincidan con esto)\n",
    "tam_latido = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo guardado\n",
    "modelo_cargado = load_model(ruta_modelo)\n",
    "print(\"Modelo cargado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde el archivo CSV\n",
    "def cargar_csv(ruta_csv):\n",
    "    datos = pd.read_csv(ruta_csv, header=None)  # Asegúrate de que no haya encabezados si son puramente datos\n",
    "    return datos.values  # Devuelve como array de NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar los datos (escalado entre 0 y 1)\n",
    "def escalar_datos_csv(datos, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        datos_escalados = scaler.fit_transform(datos)\n",
    "    else:\n",
    "        datos_escalados = scaler.transform(datos)\n",
    "    return datos_escalados, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_csv = '../data/sensor_data.csv'  # Reemplaza con la ruta a tu archivo CSV\n",
    "df = pd.read_csv(archivo_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que la columna 'value' existe en el archivo CSV\n",
    "if 'value' not in df.columns:\n",
    "    raise ValueError(\"La columna 'value' no existe en el archivo CSV.\")\n",
    "\n",
    "valores = df['value'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar el número de elementos por fila (por ejemplo, si quieres 100 elementos por fila)\n",
    "tam_fila = 100  # Ajusta esto al número que desees\n",
    "n_filas = len(valores) // tam_fila  # Número de filas completas que puedes crear\n",
    "\n",
    "# Cortamos los datos para que sea divisible entre tam_fila (si es necesario)\n",
    "valores_cortados = valores[:n_filas * tam_fila]\n",
    "\n",
    "# Reformateamos los datos en una matriz 2D (cada fila tiene 'tam_fila' elementos)\n",
    "datos_matrix = np.reshape(valores_cortados, (n_filas, tam_fila))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preprocesar los datos\n",
    "datos = datos_matrix\n",
    "datos_escalados, scaler = escalar_datos_csv(datos)\n",
    "\n",
    "# Asegurarse de que los datos tienen la forma correcta para el modelo (n_samples, tam_latido, 1)\n",
    "datos_escalados = datos_escalados.reshape(-1, tam_latido, 1)\n",
    "\n",
    "# Hacer las predicciones\n",
    "predicciones = modelo_cargado.predict(datos_escalados)\n",
    "\n",
    "# Decodificar las predicciones\n",
    "# Si las clases son 'Normal' (0) y 'Anomalía' (1), puedes obtener la clase predicha con argmax\n",
    "clases_predichas = np.argmax(predicciones, axis=1)\n",
    "\n",
    "# Si tienes más de 2 clases, puedes ajustar el codificador según tu entrenamiento\n",
    "etiquetas_predichas = ['Normal' if clase == 0 else 'Anomalía' for clase in clases_predichas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latido 1: Normal\n",
      "Latido 2: Normal\n",
      "Latido 3: Normal\n",
      "Latido 4: Normal\n",
      "Latido 5: Normal\n",
      "Latido 6: Normal\n",
      "Latido 7: Normal\n",
      "Latido 8: Normal\n",
      "Latido 9: Normal\n",
      "Latido 10: Normal\n",
      "Latido 11: Normal\n",
      "Latido 12: Anomalía\n",
      "Latido 13: Normal\n",
      "Latido 14: Normal\n",
      "Latido 15: Normal\n",
      "Latido 16: Normal\n",
      "Latido 17: Normal\n",
      "Latido 18: Normal\n",
      "Latido 19: Normal\n",
      "Latido 20: Normal\n",
      "Latido 21: Anomalía\n",
      "Latido 22: Normal\n",
      "Latido 23: Normal\n",
      "Latido 24: Normal\n",
      "Latido 25: Normal\n",
      "Latido 26: Anomalía\n",
      "Latido 27: Normal\n",
      "Latido 28: Normal\n",
      "Latido 29: Normal\n",
      "Latido 30: Normal\n",
      "Latido 31: Normal\n",
      "Latido 32: Normal\n",
      "Latido 33: Normal\n",
      "Latido 34: Normal\n",
      "Latido 35: Normal\n",
      "Latido 36: Normal\n",
      "Latido 37: Normal\n",
      "Latido 38: Normal\n",
      "Latido 39: Normal\n",
      "Latido 40: Normal\n",
      "Latido 41: Normal\n",
      "Latido 42: Normal\n",
      "Latido 43: Normal\n",
      "Latido 44: Normal\n",
      "Latido 45: Normal\n",
      "Latido 46: Normal\n",
      "Latido 47: Normal\n",
      "Latido 48: Normal\n",
      "Latido 49: Normal\n",
      "Latido 50: Normal\n",
      "Latido 51: Normal\n",
      "Latido 52: Normal\n",
      "Latido 53: Normal\n",
      "Latido 54: Normal\n",
      "Latido 55: Normal\n",
      "Predicciones guardadas en 'predicciones_ecg.csv'\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las predicciones\n",
    "for i, etiqueta in enumerate(etiquetas_predichas):\n",
    "    print(f\"Latido {i+1}: {etiqueta}\")\n",
    "\n",
    "# Si deseas guardar las predicciones en un archivo CSV\n",
    "df_predicciones = pd.DataFrame({\n",
    "    'Latido': np.arange(1, len(etiquetas_predichas) + 1),\n",
    "    'Predicción': etiquetas_predichas\n",
    "})\n",
    "df_predicciones.to_csv('predicciones_ecg.csv', index=False)\n",
    "print(\"Predicciones guardadas en 'predicciones_ecg.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
